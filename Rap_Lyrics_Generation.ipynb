{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rap Lyrics Generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import The Library For All Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import operator\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Embedding\n",
    "from numpy import array \n",
    "from keras.utils import to_categorical\n",
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pt 1: Scrape Lyrics + Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually Add Links For Each Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "url_links = [\"https://www.letssingit.com/travis-scott-lyrics-stargazing-58n9ght\",\n",
    "    \"https://www.letssingit.com/travis-scott-lyrics-carousel-j64h43v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "urls.extend(url_links)\n",
    "\n",
    "lyrics_list = []\n",
    "\n",
    "def generate_lyrics_file(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, features=\"html5lib\")\n",
    "    div = soup.find('div', {'id':'lyrics'})\n",
    "    list_lyrics = div.text.split(\"\\n\")\n",
    "    raw_lyrics = []\n",
    "    for item in list_lyrics:\n",
    "        raw_lyrics.append(item.lower())\n",
    "    return(list(filter(None, raw_lyrics)))\n",
    "\n",
    "lyrics_list = []      \n",
    "           \n",
    "for url in urls:\n",
    "        lyrics_list.extend(generate_lyrics_file(url))\n",
    "           \n",
    "lyrics_list = list(set(lyrics_list))\n",
    "with open('raw_lyrics.txt', 'w') as file_handler:\n",
    "    for item in lyrics_list:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "filename = \"raw_lyrics.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "def clean_doc(doc):\n",
    "\n",
    "    doc = doc.replace('--', ' ')\n",
    "    doc = doc.replace('-',' ')\n",
    "\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = clean_doc(raw_text)\n",
    "print('Total Sequences: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize into sequences of tokens\n",
    "length = 6 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    \n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "lines = sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pt 2: Training The LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes sequences of words to ints\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Seperate Into X & Y\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "filepath = \"weights-improvement-{epoch:02d}--{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100, callbacks=callbacks_list)\n",
    "\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pt 3: Generate Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"\" #Add most recent weights\n",
    "# Load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from random import randint\n",
    "# Load doc into memory\n",
    "def load_doc(filename):\n",
    "    # Open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # Read all text\n",
    "    text = file.read()\n",
    "    # Close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# Generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # Generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # Encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # Truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # Predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # Map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # Append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    " \n",
    "\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "#Load the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# Select a seed text\n",
    "seed_text1 = lines[randint(0,len(lines))]\n",
    "seed_text2 = lines[randint(0,len(lines))]\n",
    "seed_text3 = lines[randint(0,len(lines))]\n",
    "seed_text4 = lines[randint(0,len(lines))]\n",
    "\n",
    " \n",
    "# Generate + Print out new lines\n",
    "generated1 = generate_seq(model, tokenizer, seq_length, seed_text1, 6)\n",
    "generated2 = generate_seq(model, tokenizer, seq_length, seed_text2, 6)\n",
    "generated3 = generate_seq(model, tokenizer, seq_length, seed_text3, 6)\n",
    "generated4 = generate_seq(model, tokenizer, seq_length, seed_text4, 6)\n",
    "\n",
    "print(seed_text1 + '\\n')\n",
    "print(generated1)\n",
    "print(seed_text2 + '\\n')\n",
    "print(generated2)\n",
    "print(seed_text3 + '\\n')\n",
    "print(generated3)\n",
    "print(seed_text4 + '\\n')\n",
    "print(generated4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful tutorials:\n",
    "\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
